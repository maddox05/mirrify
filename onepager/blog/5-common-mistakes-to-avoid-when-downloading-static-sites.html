<!DOCTYPE html>
<html lang="en">
  <head>
    <!-- Google tag (gtag.js) -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-D1QS88GF1Z"
    ></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag() {
        dataLayer.push(arguments);
      }
      gtag("js", new Date());

      gtag("config", "G-D1QS88GF1Z");
    </script>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <meta
      name="description"
      content="Learn about the 5 common mistakes to avoid when downloading static sites for offline use or archiving, and how tools like Mirrify can help."
    />
    <meta
      name="keywords"
      content="static site download, website archiving, common website mistakes, offline browsing, HTML download, website preservation, digital archiving"
    />
    <title>
      5 Common Mistakes to Avoid When Downloading Static Sites | Mirrify Blog
    </title>
    <!-- Favicons -->
    <link rel="icon" href="/assets/lambda.ico" type="image/x-icon" />
    <link rel="shortcut icon" href="/assets/lambda.ico" type="image/x-icon" />
    <link rel="apple-touch-icon" href="/assets/lambda.ico" />
    <meta name="msapplication-TileImage" content="/assets/lambda.ico" />

    <script src="https://cdn.tailwindcss.com"></script>
    <style>
      html {
        font-family: "Courier New", monospace;
      }
      /* Basic styling for blog content */
      .blog-content h2 {
        font-size: 1.8rem; /* Tailwind text-2xl approx */
        font-weight: bold;
        margin-top: 2rem; /* Tailwind mt-8 approx */
        margin-bottom: 1rem; /* Tailwind mb-4 approx */
        border-bottom-width: 2px;
        border-color: #e5e7eb; /* Tailwind gray-200 approx */
        padding-bottom: 0.5rem; /* Tailwind pb-2 approx */
      }
      .blog-content h3 {
        font-size: 1.5rem; /* Tailwind text-xl approx */
        font-weight: bold;
        margin-top: 1.5rem; /* Tailwind mt-6 approx */
        margin-bottom: 0.75rem; /* Tailwind mb-3 approx */
        color: #1e3a8a; /* A darker blue, adjust as needed */
      }
      .blog-content p {
        margin-bottom: 1rem; /* Tailwind mb-4 approx */
        line-height: 1.75; /* Tailwind leading-relaxed approx */
      }
      .blog-content ul {
        list-style-type: disc;
        margin-left: 1.5rem; /* Tailwind ml-6 approx */
        margin-bottom: 1rem; /* Tailwind mb-4 approx */
      }
      .blog-content li {
        margin-bottom: 0.5rem; /* Tailwind mb-2 approx */
      }
      .blog-content a {
        color: #2563eb; /* Tailwind blue-600 approx */
        text-decoration: underline;
      }
      .blog-content a:hover {
        color: #1d4ed8; /* Tailwind blue-700 approx */
      }
      .mistake-summary {
        background-color: #eff6ff; /* Tailwind blue-50 approx */
        border-left-width: 4px;
        border-color: #3b82f6; /* Tailwind blue-500 approx */
        padding: 1.25rem; /* Tailwind p-5 approx */
        margin-bottom: 1.5rem; /* Tailwind mb-6 approx */
        border-radius: 0.375rem; /* Tailwind rounded-md approx */
      }
      .mistake-summary h3 {
        margin-top: 0;
        color: #1e40af; /* Tailwind blue-800 approx */
      }
      .references {
        margin-top: 2.5rem; /* Tailwind mt-10 approx */
        padding-top: 1rem; /* Tailwind pt-4 approx */
        border-top-width: 1px;
        border-color: #d1d5db; /* Tailwind gray-300 approx */
      }
      .references h3 {
        color: #1f2937; /* Tailwind gray-800 approx */
      }
    </style>
  </head>
  <body class="bg-neutral-100 text-neutral-900">
    <div class="max-w-4xl mx-auto px-6 py-12">
      <header class="mb-12 flex justify-between items-start">
        <div>
          <h1
            class="text-4xl font-bold tracking-tighter uppercase border-b-4 border-black inline-block pb-1"
          >
            <a href="/blog/" class="text-neutral-900">Mirrify Blog</a>
          </h1>
        </div>
        <nav class="pt-2">
          <ul class="flex space-x-4 font-bold">
            <li>
              <a
                href="/"
                class="px-3 py-2 text-neutral-900 hover:bg-black hover:text-white transition"
                >Home</a
              >
            </li>
            <li>
              <a
                href="/docs/index.html"
                class="px-3 py-2 text-neutral-900 hover:bg-black hover:text-white transition"
                >Docs</a
              >
            </li>
          </ul>
        </nav>
      </header>

      <main
        class="bg-white p-8 md:p-10 border-4 border-black shadow-lg rounded-xl"
      >
        <article class="blog-content">
          <h1 class="text-3xl md:text-4xl font-bold mb-3">
            5 Common Mistakes to Avoid When Downloading Static Sites
          </h1>
          <p class="text-sm text-neutral-600 mb-8">
            <em>Published: May 25, 2025</em>
          </p>

          <p>
            Downloading static websites, whether for offline browsing, archival
            purposes, or development, seems straightforward. However, several
            common pitfalls can lead to incomplete downloads, broken layouts, or
            unusable archives. Understanding these mistakes can save you time
            and frustration. This article explores five such common errors and
            offers insights into how to avoid them, with a brief mention of how
            tools like Mirrify aim to simplify this process.
          </p>

          <h2>Watch: Website Archiving Tutorial</h2>
          <p>
            Before diving into the common mistakes, let's watch a practical
            tutorial on how to properly archive websites using command-line
            tools like wget:
          </p>

          <div class="mb-8">
            <div
              style="
                position: relative;
                padding-bottom: 56.25%;
                height: 0;
                overflow: hidden;
              "
            >
              <iframe
                style="
                  position: absolute;
                  top: 0;
                  left: 0;
                  width: 100%;
                  height: 100%;
                "
                src="https://www.youtube.com/embed/GWlcpRMWM2E"
                title="How to Download a Website with httrack"
                frameborder="0"
                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                allowfullscreen
              >
              </iframe>
            </div>
            <p class="text-sm text-neutral-600 mt-2">
              <em
                >Tutorial: How to create an offline copy of a website using
                httrack</em
              >
            </p>
          </div>

          <section class="mistake-summary">
            <h3>Mistake 1: Ignoring robots.txt or Overloading Servers</h3>
            <p>
              Many websites use a <code>robots.txt</code> file to guide web
              crawlers, specifying which parts of the site should not be
              accessed. While your intention might be personal archiving,
              aggressively downloading a site without respecting
              <code>robots.txt</code> (if applicable to your use case and the
              tool you use) or making too many rapid requests can overload the
              server. This can lead to your IP being blocked or, in a worst-case
              scenario, disrupting the site for other users. Always aim to be a
              "good netizen."
            </p>
            <p>
              <strong>Solution:</strong> Use downloaders that can be configured
              to respect <code>robots.txt</code> (many do by default for ethical
              reasons), and always introduce delays between requests. If a site
              offers an API or a dedicated download option, prefer that. Tools
              designed for site mirroring often have settings to control
              download speed and concurrency.
            </p>
          </section>

          <section class="mistake-summary">
            <h3>
              Mistake 2: Mishandling Dynamic Content and JavaScript-Generated
              HTML
            </h3>
            <p>
              Modern websites heavily rely on JavaScript to load content
              dynamically. A simple HTML downloader might only grab the initial
              HTML scaffold, missing content that's loaded or rendered by
              client-side scripts. This results in an incomplete or
              non-functional offline copy.
            </p>
            <p>
              <strong>Solution:</strong> Ensure your chosen tool can execute
              JavaScript or is specifically designed for "scraping" dynamic
              sites. This often involves using a headless browser engine (like
              Puppeteer or Selenium) under the hood. Some dedicated site
              downloaders are better equipped to handle JavaScript-heavy sites
              than generic command-line tools like <code>wget</code> or
              <code>curl</code> used naively.
            </p>
          </section>

          <section class="mistake-summary">
            <h3>
              Mistake 3: Incorrectly Managing Asset Paths (CSS, JS, Images)
            </h3>
            <p>
              A frequent issue is that downloaded sites have broken styling,
              missing images, or non-functional scripts because the paths to
              these assets are incorrect in the local copy. Websites often use
              absolute paths (e.g., <code>/css/style.css</code>) or
              domain-relative paths that don't translate well when the site is
              viewed locally from a different directory structure (e.g.,
              <code>file:///C:/Users/YourName/Downloads/MySite/index.html</code
              >).
            </p>
            <p>
              <strong>Solution:</strong> Good site downloaders automatically
              rewrite these paths to be relative to the downloaded files,
              ensuring everything links correctly. Understanding the difference
              between absolute and relative paths is crucial. For instance, an
              absolute path like
              <code><img src="/images/logo.png" /></code> assumes the image is
              at the root of the web server. When offline, this path is
              meaningless unless the downloaded site structure perfectly mimics
              the server's root. Relative paths (e.g.,
              <code><img src="images/logo.png" /></code> or
              <code><img src="../images/logo.png" /></code>) are generally more
              robust for offline viewing as they are resolved based on the
              current file's location.
            </p>
          </section>

          <section class="mistake-summary">
            <h3>
              Mistake 4: Not Accounting for Character Encoding and Special
              Characters
            </h3>
            <p>
              Web pages can use various character encodings (UTF-8, ISO-8859-1,
              etc.). If the downloader doesn't handle these correctly, or if the
              <code>&lt;meta charset&gt;</code> tag is missing or
              misinterpreted, text can appear garbled. Similarly, file names
              with special characters or very long paths can cause issues on
              certain operating systems if not properly sanitized during the
              download process.
            </p>
            <p>
              <strong>Solution:</strong> Ensure your tool correctly interprets
              and preserves character encoding. Modern tools usually default to
              UTF-8, which is widely compatible. For file names, good
              downloaders will sanitize them to be OS-friendly, replacing or
              encoding problematic characters and truncating overly long names
              while trying to maintain link integrity.
            </p>
          </section>

          <section class="mistake-summary">
            <h3>Mistake 5: Incomplete Downloads or Missing Pages</h3>
            <p>
              Sometimes, downloads are simply incomplete. This could be due to
              network interruptions, server-side limitations (like download caps
              or timeouts), or the downloader not being configured to crawl deep
              enough or follow all necessary link types (e.g., links embedded in
              CSS or JavaScript).
            </p>
            <p>
              <strong>Solution:</strong> Use a downloader that has robust error
              handling and resume capabilities. Configure the crawl depth
              appropriately for the site's structure. Check logs for any errors
              or missed resources. For very large or complex sites, it might be
              necessary to download in stages or use more specialized archival
              tools. Some tools also allow specifying URL patterns to include or
              exclude, helping to focus the download on relevant content only.
            </p>
          </section>

          <h2>Popular Tools for Website Archiving</h2>
          <p>
            Understanding the available tools can help you choose the right
            approach for your specific needs. Each tool has its strengths and is
            suited for different scenarios:
          </p>

          <h3>Command-Line Tools</h3>
          <ul>
            <li>
              <strong>wget:</strong> The classic Unix tool, excellent for
              recursive downloads and has extensive options for customization.
              Great for technical users who want fine control over the download
              process.
            </li>
            <li>
              <strong>curl:</strong> More focused on single requests but very
              powerful for API interactions and custom header manipulation.
            </li>
            <li>
              <strong>HTTrack:</strong> Cross-platform website copier with both
              GUI and command-line interfaces. Known for good handling of
              complex site structures.
            </li>
          </ul>

          <h3>Browser-Based Solutions</h3>
          <ul>
            <li>
              <strong>Browser Extensions:</strong> Tools like "Save Page WE" or
              "SingleFile" can capture complete pages including rendered
              JavaScript content.
            </li>
            <li>
              <strong>Print to PDF:</strong> While limited, this can be useful
              for preserving the visual layout of important pages.
            </li>
            <li>
              <strong>Browser Developer Tools:</strong> Can be used to save
              resources manually, though this approach doesn't scale well.
            </li>
          </ul>

          <h3>Specialized Archiving Tools</h3>
          <ul>
            <li>
              <strong>Archive-It:</strong> Professional web archiving service
              used by libraries and institutions.
            </li>
            <li>
              <strong>Heritrix:</strong> Open-source web crawler developed by
              the Internet Archive, designed for large-scale archiving.
            </li>
            <li>
              <strong>Webrecorder:</strong> User-friendly tool for creating
              high-fidelity web archives, especially good for interactive
              content.
            </li>
          </ul>

          <h2>Best Practices for Ethical Website Archiving</h2>
          <p>
            When downloading websites, it's crucial to follow ethical guidelines
            that respect both the website owners and other users. Here are key
            principles to follow:
          </p>

          <h3>Respect Server Resources</h3>
          <ul>
            <li>
              <strong>Implement delays:</strong> Always add delays between
              requests (typically 1-10 seconds) to avoid overwhelming the
              server.
            </li>
            <li>
              <strong>Limit concurrent connections:</strong> Use only one or two
              simultaneous connections to avoid appearing like a DDoS attack.
            </li>
            <li>
              <strong>Choose appropriate times:</strong> Avoid downloading
              during peak hours when the server is likely to be busiest.
            </li>
            <li>
              <strong>Monitor impact:</strong> If a site becomes slow or
              unresponsive, pause your download and try again later.
            </li>
          </ul>

          <h3>Follow Website Policies</h3>
          <ul>
            <li>
              <strong>Check robots.txt:</strong> Always review the robots.txt
              file to understand the site's crawling preferences.
            </li>
            <li>
              <strong>Read Terms of Service:</strong> Some sites explicitly
              prohibit bulk downloading or archiving.
            </li>
            <li>
              <strong>Respect copyright:</strong> Only archive content for
              personal use, research, or preservation purposes.
            </li>
            <li>
              <strong>Contact site owners:</strong> For large sites or
              commercial purposes, consider reaching out to get permission.
            </li>
          </ul>

          <h3>Technical Considerations</h3>
          <ul>
            <li>
              <strong>Use appropriate user agents:</strong> Identify your tool
              and include contact information in case of issues.
            </li>
            <li>
              <strong>Handle errors gracefully:</strong> Implement retry logic
              with exponential backoff for failed requests.
            </li>
            <li>
              <strong>Preserve metadata:</strong> Save creation dates, last
              modified times, and other important metadata.
            </li>
            <li>
              <strong>Document your process:</strong> Keep records of what was
              archived, when, and with what settings.
            </li>
          </ul>

          <h2>Advanced Archiving Techniques</h2>
          <p>
            For complex websites or specialized needs, advanced techniques can
            help create more complete and useful archives:
          </p>

          <h3>Handling Dynamic Content</h3>
          <p>
            Modern websites often rely heavily on JavaScript to load content
            dynamically. Traditional tools like wget only capture the initial
            HTML response, missing content that loads after the page renders.
          </p>
          <ul>
            <li>
              <strong>Headless browsers:</strong> Tools like Puppeteer or
              Selenium can execute JavaScript and capture the fully rendered
              page.
            </li>
            <li>
              <strong>Wait strategies:</strong> Implement delays or wait for
              specific elements to load before capturing content.
            </li>
            <li>
              <strong>User interaction simulation:</strong> Some content only
              loads after user actions like clicking or scrolling.
            </li>
            <li>
              <strong>API archiving:</strong> For content-heavy sites, consider
              archiving the underlying APIs that feed the frontend.
            </li>
          </ul>

          <h3>Dealing with Authentication</h3>
          <p>
            Some content requires login credentials or API keys to access.
            Here's how to handle authenticated content ethically:
          </p>
          <ul>
            <li>
              <strong>Session management:</strong> Maintain login sessions
              throughout the archiving process.
            </li>
            <li>
              <strong>Cookie preservation:</strong> Save and reuse
              authentication cookies for consistent access.
            </li>
            <li>
              <strong>Rate limiting:</strong> Be extra careful with
              authenticated requests as they're more easily traced back to you.
            </li>
            <li>
              <strong>Personal content only:</strong> Only archive content you
              have legitimate access to view.
            </li>
          </ul>

          <h3>Optimizing Archive Quality</h3>
          <ul>
            <li>
              <strong>Multiple quality levels:</strong> Archive both
              high-resolution images and compressed versions for different use
              cases.
            </li>
            <li>
              <strong>Redundant storage:</strong> Keep multiple copies of
              important archives in different formats and locations.
            </li>
            <li>
              <strong>Validation scripts:</strong> Create scripts to verify that
              your archives are complete and functional.
            </li>
            <li>
              <strong>Regular updates:</strong> For ongoing archival projects,
              implement systems to detect and archive changes.
            </li>
          </ul>

          <h2>Common wget Command Examples</h2>
          <p>
            Since wget is one of the most popular tools for website archiving,
            here are some practical command examples for different scenarios:
          </p>

          <h3>Basic Site Mirror</h3>
          <pre
            style="
              background-color: #f3f4f6;
              padding: 1rem;
              border-radius: 0.375rem;
              overflow-x: auto;
              font-family: monospace;
              margin: 1rem 0;
            "
          ><code>wget --mirror --page-requisites --html-extension --convert-links --domains example.com --wait=1 https://example.com</code></pre>

          <h3>Polite Archiving with Delays</h3>
          <pre
            style="
              background-color: #f3f4f6;
              padding: 1rem;
              border-radius: 0.375rem;
              overflow-x: auto;
              font-family: monospace;
              margin: 1rem 0;
            "
          ><code>wget --recursive --level=5 --wait=2 --random-wait --user-agent="Mozilla/5.0 (compatible; Archiver)" --domains example.com https://example.com</code></pre>

          <h3>Creating WARC Files for Professional Archiving</h3>
          <pre
            style="
              background-color: #f3f4f6;
              padding: 1rem;
              border-radius: 0.375rem;
              overflow-x: auto;
              font-family: monospace;
              margin: 1rem 0;
            "
          ><code>wget --mirror --warc-file=example_archive --warc-cdx --page-requisites --convert-links --domains example.com --wait=3 https://example.com</code></pre>

          <h3>Excluding Unwanted Content</h3>
          <pre
            style="
              background-color: #f3f4f6;
              padding: 1rem;
              border-radius: 0.375rem;
              overflow-x: auto;
              font-family: monospace;
              margin: 1rem 0;
            "
          ><code>wget --mirror --reject="*.mp4,*.avi,*.mov" --exclude-directories="/admin,/private" --domains example.com https://example.com</code></pre>

          <h2>Troubleshooting Common Archive Issues</h2>
          <p>
            Even with the best preparation, you may encounter issues during the
            archiving process. Here's how to diagnose and fix common problems:
          </p>

          <h3>Incomplete Downloads</h3>
          <ul>
            <li>
              <strong>Check log files:</strong> Most tools provide detailed logs
              showing what succeeded and what failed.
            </li>
            <li>
              <strong>Network timeouts:</strong> Increase timeout values for
              slow connections or large files.
            </li>
            <li>
              <strong>Depth limitations:</strong> Ensure your crawl depth is
              sufficient to reach all desired content.
            </li>
            <li>
              <strong>Resume capability:</strong> Use tools that can resume
              interrupted downloads.
            </li>
          </ul>

          <h3>Broken Links in Archives</h3>
          <ul>
            <li>
              <strong>Path rewriting issues:</strong> Verify that absolute URLs
              are correctly converted to relative paths.
            </li>
            <li>
              <strong>Missing assets:</strong> Check that CSS, JavaScript, and
              image files were successfully downloaded.
            </li>
            <li>
              <strong>External dependencies:</strong> Some sites rely on
              external resources that may not be archived.
            </li>
            <li>
              <strong>URL encoding problems:</strong> Special characters in URLs
              may cause issues with some tools.
            </li>
          </ul>

          <h3>Performance Issues</h3>
          <ul>
            <li>
              <strong>Memory usage:</strong> Large sites may require tools with
              better memory management.
            </li>
            <li>
              <strong>Disk space:</strong> Monitor available storage and plan
              for significantly larger archives than expected.
            </li>
            <li>
              <strong>Bandwidth limitations:</strong> Implement rate limiting to
              avoid overwhelming your network connection.
            </li>
            <li>
              <strong>Processing time:</strong> Large sites can take days or
              weeks to archive completely.
            </li>
          </ul>

          <h2>Legal and Copyright Considerations</h2>
          <p>
            Website archiving intersects with various legal considerations that
            vary by jurisdiction. While this article cannot provide legal
            advice, here are important factors to consider:
          </p>

          <h3>Fair Use and Personal Archives</h3>
          <ul>
            <li>
              <strong>Personal use exemptions:</strong> Many jurisdictions allow
              personal copying for private use, research, or study.
            </li>
            <li>
              <strong>Educational purposes:</strong> Academic institutions may
              have broader rights for archival and research purposes.
            </li>
            <li>
              <strong>Transformative use:</strong> Using archived content for
              analysis, commentary, or research may qualify for fair use
              protections.
            </li>
            <li>
              <strong>Geographic variations:</strong> Copyright laws vary
              significantly between countries and regions.
            </li>
          </ul>

          <h3>Risk Mitigation Strategies</h3>
          <ul>
            <li>
              <strong>Keep archives private:</strong> Don't share or
              redistribute archived content without permission.
            </li>
            <li>
              <strong>Document legitimate purposes:</strong> Maintain clear
              records of why you're archiving specific content.
            </li>
            <li>
              <strong>Respect takedown requests:</strong> If contacted by
              content owners, respond promptly and appropriately.
            </li>
            <li>
              <strong>Seek permission when possible:</strong> For important or
              commercial uses, consider requesting explicit permission.
            </li>
          </ul>
          <h2>How Tools Like Mirrify Can Help</h2>
          <p>
            While the above points highlight common challenges, dedicated tools
            like Mirrify are designed to address many of these issues. Mirrify
            aims to provide a user-friendly experience for creating complete,
            browseable offline copies of static and JavaScript-heavy websites.
            It focuses on:
          </p>
          <ul>
            <li>
              <strong>Faithful Archiving:</strong> Capturing not just HTML, but
              also CSS, JavaScript, images, and other assets.
            </li>
            <li>
              <strong>Path Rewriting:</strong> Automatically adjusting links and
              asset paths to ensure the downloaded site works correctly offline.
            </li>
            <li>
              <strong>Handling Dynamic Content:</strong> Employing techniques to
              capture content generated by JavaScript.
            </li>
            <li>
              <strong>User Control:</strong> Offering options to manage download
              behavior, though aiming for smart defaults.
            </li>
          </ul>
          <p>
            The goal of such tools is to abstract away the complexities,
            allowing users to archive websites without needing to become experts
            in web protocols or command-line utilities. While no tool is perfect
            for every scenario, using a purpose-built solution can significantly
            reduce the chances of falling into these common traps.
          </p>

          <p>
            By being aware of these potential pitfalls and choosing the right
            tools, you can ensure your website downloads are complete,
            functional, and truly useful for your intended purpose.
          </p>

          <section class="references">
            <h3>References & Further Reading</h3>
            <ul>
              <li>
                W3C. (n.d.). <em>Relative URLs</em>. Retrieved from
                <a
                  href="https://www.w3.org/TR/WD-html40-970917/htmlweb.html#h-5.1.2"
                  target="_blank"
                  rel="noopener noreferrer"
                  >https://www.w3.org/TR/WD-html40-970917/htmlweb.html#h-5.1.2</a
                >
              </li>
              <li>
                Google Search Central. (n.d.).
                <em>Robots.txt Specifications</em>. Retrieved from
                <a
                  href="https://developers.google.com/search/docs/advanced/robots/robots_txt"
                  target="_blank"
                  rel="noopener noreferrer"
                  >https://developers.google.com/search/docs/advanced/robots/robots_txt</a
                >
              </li>
              <li>
                MDN Web Docs. (n.d.). <em>Character encodings</em>. Retrieved
                from
                <a
                  href="https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta/charset"
                  target="_blank"
                  rel="noopener noreferrer"
                  >https://developer.mozilla.org/en-US/docs/Web/HTML/Element/meta/charset</a
                >
              </li>
              <li>
                Conductor. (n.d.).
                <em>Absolute vs Relative URLs: when to use which for SEO?</em>
                Retrieved from
                <a
                  href="https://www.conductor.com/academy/urls/faq/absolute-vs-relative/"
                  target="_blank"
                  rel="noopener noreferrer"
                  >https://www.conductor.com/academy/urls/faq/absolute-vs-relative/</a
                >
              </li>
            </ul>
          </section>
        </article>
      </main>

      <footer class="mt-16 border-t-2 border-black pt-8 text-center text-sm">
        <p>Â© 2025 Mirrify. All rights reserved.</p>
        <p class="mt-4">
          <a href="/blog/" class="underline hover:text-neutral-600"
            >Back to Blog</a
          >
        </p>
      </footer>
    </div>
  </body>
</html>
